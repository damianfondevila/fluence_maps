{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_create_tfrecords.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNZwyusgPTSVS1l/E9RD5SE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablojrios/fluence_maps/blob/master/create_tfrecords/tf2_create_tfrecords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJx5w948nCcj",
        "colab_type": "code",
        "outputId": "f72ec990-eff8-4bc3-fd3d-3d1924264c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "import os\n",
        "%cd -q '/content'\n",
        "if os.path.exists('fluence_maps'):\n",
        "  !rm -fr fluence_maps\n",
        "if os.path.exists('lodgepole'):\n",
        "  !rm -fr lodgepole\n",
        "\n",
        "## Install required dependencies\n",
        "!pip install -q pydicom\n",
        "\n",
        "!git clone https://gitlab.com/brohrer/lodgepole.git\n",
        "!pip install -e lodgepole\n",
        "\n",
        "GIT_USERNAME = \"pablojrios\"\n",
        "GIT_TOKEN = \"1d88a0b85d2b00a03796e4d8b7e5f7b249b12f9b\"\n",
        "!git clone -s https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/fluence_maps.git"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'lodgepole'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects:  12% (1/8)\u001b[K\rremote: Counting objects:  25% (2/8)\u001b[K\rremote: Counting objects:  37% (3/8)\u001b[K\rremote: Counting objects:  50% (4/8)\u001b[K\rremote: Counting objects:  62% (5/8)\u001b[K\rremote: Counting objects:  75% (6/8)\u001b[K\rremote: Counting objects:  87% (7/8)\u001b[K\rremote: Counting objects: 100% (8/8)\u001b[K\rremote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects:  12% (1/8)\u001b[K\rremote: Compressing objects:  25% (2/8)\u001b[K\rremote: Compressing objects:  37% (3/8)\u001b[K\rremote: Compressing objects:  50% (4/8)\u001b[K\rremote: Compressing objects:  62% (5/8)\u001b[K\rremote: Compressing objects:  75% (6/8)\u001b[K\rremote: Compressing objects:  87% (7/8)\u001b[K\rremote: Compressing objects: 100% (8/8)\u001b[K\rremote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 49 (delta 1), reused 0 (delta 0), pack-reused 41\u001b[K\n",
            "Unpacking objects:   2% (1/49)   \rUnpacking objects:   4% (2/49)   \rUnpacking objects:   6% (3/49)   \rUnpacking objects:   8% (4/49)   \rUnpacking objects:  10% (5/49)   \rUnpacking objects:  12% (6/49)   \rUnpacking objects:  14% (7/49)   \rUnpacking objects:  16% (8/49)   \rUnpacking objects:  18% (9/49)   \rUnpacking objects:  20% (10/49)   \rUnpacking objects:  22% (11/49)   \rUnpacking objects:  24% (12/49)   \rUnpacking objects:  26% (13/49)   \rUnpacking objects:  28% (14/49)   \rUnpacking objects:  30% (15/49)   \rUnpacking objects:  32% (16/49)   \rUnpacking objects:  34% (17/49)   \rUnpacking objects:  36% (18/49)   \rUnpacking objects:  38% (19/49)   \rUnpacking objects:  40% (20/49)   \rUnpacking objects:  42% (21/49)   \rUnpacking objects:  44% (22/49)   \rUnpacking objects:  46% (23/49)   \rUnpacking objects:  48% (24/49)   \rUnpacking objects:  51% (25/49)   \rUnpacking objects:  53% (26/49)   \rUnpacking objects:  55% (27/49)   \rUnpacking objects:  57% (28/49)   \rUnpacking objects:  59% (29/49)   \rUnpacking objects:  61% (30/49)   \rUnpacking objects:  63% (31/49)   \rUnpacking objects:  65% (32/49)   \rUnpacking objects:  67% (33/49)   \rUnpacking objects:  69% (34/49)   \rUnpacking objects:  71% (35/49)   \rUnpacking objects:  73% (36/49)   \rUnpacking objects:  75% (37/49)   \rUnpacking objects:  77% (38/49)   \rUnpacking objects:  79% (39/49)   \rUnpacking objects:  81% (40/49)   \rUnpacking objects:  83% (41/49)   \rUnpacking objects:  85% (42/49)   \rUnpacking objects:  87% (43/49)   \rUnpacking objects:  89% (44/49)   \rUnpacking objects:  91% (45/49)   \rUnpacking objects:  93% (46/49)   \rUnpacking objects:  95% (47/49)   \rUnpacking objects:  97% (48/49)   \rUnpacking objects: 100% (49/49)   \rUnpacking objects: 100% (49/49), done.\n",
            "Obtaining file:///content/lodgepole\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lodgepole==3) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lodgepole==3) (1.18.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lodgepole==3) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lodgepole==3) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lodgepole==3) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lodgepole==3) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->lodgepole==3) (1.12.0)\n",
            "Installing collected packages: lodgepole\n",
            "  Found existing installation: lodgepole 3\n",
            "    Can't uninstall 'lodgepole'. No files were found to uninstall.\n",
            "  Running setup.py develop for lodgepole\n",
            "Successfully installed lodgepole\n",
            "Cloning into 'fluence_maps'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 53 (delta 28), reused 17 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvsvrcRHb7b3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import lodgepole.image_tools as lit doesn't work, the following is equivalent\n",
        "# from importlib.machinery import SourceFileLoader\n",
        "# somemodule = SourceFileLoader('lit', '/content/lodgepole/lodgepole/image_tools.py').load_module()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "%cd -q '/content/fluence_maps/create_tfrecords'\n",
        "from dataset_utils import _dataset_exists, _get_filenames_and_gamma_values, _convert_dataset\n",
        "from sklearn.utils import shuffle\n",
        "from os import path\n",
        "from tf2_oversampling_dicom_files import do_oversampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_KVdeEFcCVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91331e21-2ab8-4ac5-93b5-4c5b2e43f99d"
      },
      "source": [
        "print('Tensorflow version = {}'.format(tf.__version__))\n",
        "print('Executing eagerly = {}'.format(tf.executing_eagerly()))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version = 2.2.0-rc3\n",
            "Executing eagerly = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wgXL1PXBhEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===============================================DEFINE YOUR ARGUMENTS=================================================\n",
        "ARG_DATASET_DIR='/hdd/data/radioterapia/ciolaplata'\n",
        "# The number of shards to split the dataset into\n",
        "ARG_NUM_SHARDS=4\n",
        "ARG_VALIDATION_SIZE=0.2\n",
        "# Seed for repeatability.\n",
        "ARG_RANDOM_SEED=113355\n",
        "# folder under ARG_DATASET_DIR path.\n",
        "ARG_TFDATASET_FOLDER='tfds.2019.ovs'\n",
        "# file with gamma values under ARG_DATASET_DIR path.\n",
        "ARG_DICOM_AND_GAMMA_CSV='codex.2019.corregidos.2.csv'\n",
        "# ARG_IMAGE_TYPE: 0 - RGB; 1 - Grayscale: Convert color images to 3D grayscale images (channel is repeated 3 times);\n",
        "# 2 - Dicom\n",
        "ARG_IMAGE_TYPE=2\n",
        "# if False only training and validation partition are created.\n",
        "ARG_TEST_PARTITION=False\n",
        "# if True copy of images is performed.\n",
        "ARG_OVERSAMPLING=True\n",
        "ARG_OVERSAMPLING_GAMMA_THRESHOLD = 99.8 # percentage\n",
        "ARG_OVERSAMPLING_FACTOR = 0.9 # 1 is 100%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWE93zltBnR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#=================================================CHECKS==============================================\n",
        "# Check if there is a dataset directory entered\n",
        "if ARG_DATASET_DIR == \"\":\n",
        "    raise ValueError('dataset_dir is empty. Please state a dataset_dir argument.')\n",
        "\n",
        "if ARG_TFDATASET_FOLDER == \"\":\n",
        "    raise ValueError('tfdataset_folder is empty. Please state a tfdataset_dir argument.')\n",
        "\n",
        "# If the TFRecord files already exist in the directory, then exit without creating the files again\n",
        "tfdataset_dir = path.join(ARG_DATASET_DIR, ARG_TFDATASET_FOLDER)\n",
        "if _dataset_exists(dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS):\n",
        "    print('Dataset files already exist. Exiting without re-creating them.')\n",
        "    return None\n",
        "\n",
        "elif not tf.io.gfile.exists(tfdataset_dir):\n",
        "    tf.io.gfile.mkdir(tfdataset_dir)\n",
        "\n",
        "print(\"Reading images from {} and writing TF records to {}\".format(ARG_DATASET_DIR, tfdataset_dir))\n",
        "\n",
        "#==============================================================END OF CHECKS===================================================================\n",
        "# Get a pandas dataframe of image full filenames and gamma indeces values.\n",
        "df_dcm_out = _get_filenames_and_gamma_values(ARG_DICOM_AND_GAMMA_CSV, ARG_DATASET_DIR)\n",
        "\n",
        "# Find the number of validation examples we need\n",
        "num_validation = int(ARG_VALIDATION_SIZE * len(df_dcm_out))\n",
        "\n",
        "print(f'\\nrandom seed partition = {ARG_RANDOM_SEED}')\n",
        "# Divide the training datasets into train and test:\n",
        "df_dcm_out = shuffle(df_dcm_out, random_state=ARG_RANDOM_SEED)\n",
        "\n",
        "# convert to list because a dataframe column is of type pandas...Series\n",
        "if not ARG_TEST_PARTITION:\n",
        "    df_training = df_dcm_out[num_validation:]\n",
        "    df_validation = df_dcm_out[:num_validation]\n",
        "else:\n",
        "    df_training = df_dcm_out[num_validation*2:]\n",
        "    df_validation = df_dcm_out[:num_validation]\n",
        "    df_testing = df_dcm_out[num_validation:num_validation*2]\n",
        "\n",
        "# Hacer oversampling de mapas menores o iguales a un valor de gamma en df_training\n",
        "if ARG_OVERSAMPLING:\n",
        "    df_training = do_oversampling(df_training, ARG_OVERSAMPLING_GAMMA_THRESHOLD, ARG_OVERSAMPLING_FACTOR)\n",
        "\n",
        "# convert to list because a dataframe column is of type pandas...Series\n",
        "if not ARG_TEST_PARTITION:\n",
        "    training_filenames = df_training['dicom_full_filepath'].to_list()\n",
        "    validation_filenames = df_validation['dicom_full_filepath'].to_list()\n",
        "    training_gamma = df_training['gamma_index'].to_list()\n",
        "    validation_gamma = df_validation['gamma_index'].to_list()\n",
        "else:\n",
        "    training_filenames = df_training['dicom_full_filepath'].to_list()\n",
        "    validation_filenames = df_validation['dicom_full_filepath'].to_list()\n",
        "    testing_filenames = df_testing['dicom_full_filepath'].to_list()\n",
        "    training_gamma = df_training['gamma_index'].to_list()\n",
        "    validation_gamma = df_validation['gamma_index'].to_list()\n",
        "    testing_gamma = df_testing['gamma_index'].to_list()\n",
        "\n",
        "# First, convert the training and validation sets.\n",
        "_convert_dataset('train', training_filenames, training_gamma,\n",
        "                  dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS, image_type = ARG_IMAGE_TYPE)\n",
        "\n",
        "if num_validation > 0:\n",
        "    _convert_dataset('validation', validation_filenames, validation_gamma,\n",
        "                      dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS, image_type = ARG_IMAGE_TYPE)\n",
        "\n",
        "    if not not ARG_TEST_PARTITION:\n",
        "        _convert_dataset('test', testing_filenames, testing_gamma,\n",
        "                          dataset_dir = tfdataset_dir, _NUM_SHARDS = ARG_NUM_SHARDS, image_type = ARG_IMAGE_TYPE)\n",
        "\n",
        "print('\\nFinished converting the dataset!')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}